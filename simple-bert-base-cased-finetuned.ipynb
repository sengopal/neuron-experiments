{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e11b2ce1",
   "metadata": {},
   "source": [
    "# Compiling and Deploying HuggingFace Pretrained BERT on Trn1 or Inf2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59a44364",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will compile and deploy a HuggingFace ü§ó Transformers BERT model for accelerated inference on Neuron. In this tutorial, we will be deploying directly on Trn1/Inf2 instances. If you are looking to deploy this model through SageMaker on Inf2 instance, please visit the [Sagemaker samples repository](https://github.com/aws-neuron/aws-neuron-sagemaker-samples/tree/master/inference/inf2-bert-on-sagemaker). \n",
    "\n",
    "This tutorial will use the [bert-base-cased-finetuned-mrpc](https://huggingface.co/bert-base-cased-finetuned-mrpc) model. This model has 12 layers, 768 hidden dimensions, 12 attention heads, and 110M total parameters. The final layer is a binary classification head that has been trained on the Microsoft Research Paraphrase Corpus (`mrpc`). The input to the model is two sentences and the output of the model is whether or not those sentences are a paraphrase of each other. \n",
    "\n",
    "This tutorial has the following main sections:\n",
    "\n",
    "1. Install dependencies\n",
    "1. Compile the BERT model\n",
    "1. Run inference on Neuron and compare results to CPU\n",
    "1. Benchmark the model using multicore inference\n",
    "1. Finding the optimal batch size\n",
    "\n",
    "This Jupyter notebook should be run on a Trn1 instance (`trn1.2xlarge` or larger.) or Inf2 instance (`inf2.xlarge` or larger.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ceecb92",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "The code in this tutorial is written for Jupyter Notebooks. To use Jupyter Notebook on the Neuron instance, you\n",
    "can use this [guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html).\n",
    "\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `transformers`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the Trn1/Inf2 setup guide. The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66392b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=True #Supresses tokenizer warnings making errors easier to detect\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from transformers) (3.16.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from requests->transformers) (3.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/neuron_pt21/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m169.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n",
      "Downloading regex-2024.9.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m782.0/782.0 kB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "Downloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m194.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.7 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=True #Supresses tokenizer warnings making errors easier to detect\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82533d8e",
   "metadata": {},
   "source": [
    "## Compile the model into an AWS Neuron optimized TorchScript\n",
    "\n",
    "In the following section, we load the BERT model and tokenizer, get a sample input, run inference on CPU, compile the model for Neuron using `torch_neuronx.trace()`, and save the optimized model as `TorchScript`.\n",
    "\n",
    "`torch_neuronx.trace()` expects a tensor or tuple of tensor inputs to use for tracing, so we unpack the tokenizer output using the `encode` function. \n",
    "\n",
    "The result of the trace stage will be a static executable where the operations to be run upon inference are determined during compilation. This means that when inferring, the resulting Neuron model must be executed with tensors that are the exact same shape as those provided at compilation time. If a model is given a tensor at inference time whose shape does not match the tensor given at compilation time, an error will occur.\n",
    "\n",
    "For language models, the shape of the tokenizer tensors can vary based on the length of input sentence. We can satisfy the Neuron restriction of using a fixed shape input by padding all varying input tensors to a specified length. In a deployment scenario, the padding size should be chosen based on the maximum token length that is expected to occur for the application.\n",
    "\n",
    "In the following section we will assume that we will receive a maximum of 128 tokens at inference time. We will pad our example inputs by using `padding='max_length'` and to avoid potential errors caused by creating a tensor that is larger than `max_length=128`, we will always tokenize using `truncation=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9aac5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/neuron_pt21/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "\n",
    "\n",
    "def encode(tokenizer, *inputs, max_length=128, batch_size=1):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        *inputs,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return (\n",
    "        torch.repeat_interleave(tokens['input_ids'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['attention_mask'], batch_size, 0),\n",
    "        torch.repeat_interleave(tokens['token_type_ids'], batch_size, 0),\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the tokenizer and model\n",
    "name = \"bert-base-cased-finetuned-mrpc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name, torchscript=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd848d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "paraphrase = encode(tokenizer, sequence_0, sequence_2)\n",
    "not_paraphrase = encode(tokenizer, sequence_0, sequence_1)\n",
    "\n",
    "# Run the original PyTorch BERT model on CPU\n",
    "cpu_paraphrase_logits = model(*paraphrase)[0]\n",
    "cpu_not_paraphrase_logits = model(*not_paraphrase)[0]\n",
    "\n",
    "# Compile the model for Neuron\n",
    "model_neuron = torch_neuronx.trace(model, paraphrase)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'model.pt'\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9605d",
   "metadata": {},
   "source": [
    "## Run inference and compare results\n",
    "\n",
    "In this section we load the compiled model, run inference on Neuron, and compare the CPU and Neuron outputs.\n",
    "\n",
    "NOTE: Although this tutorial section uses one NeuronCore (and the next section uses two NeuronCores), by default each Jupyter notebook Python process will attempt to take ownership of all NeuronCores visible on the instance. For multi-process applications where each process should only use a subset of the NeuronCores on the instance you can use NEURON_RT_NUM_CORES=N or NEURON_RT_VISIBLE_CORES=< list of NeuronCore IDs > when starting the Jupyter notebook as described in [NeuronCore Allocation and Model Placement for Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/inference/core-placement.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d509aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU paraphrase logits:         [[-0.34945565  1.9003879 ]]\n",
      "Neuron paraphrase logits:     [[-0.34909704  1.8992746 ]]\n",
      "CPU not-paraphrase logits:     [[ 0.53863674 -2.219714  ]]\n",
      "Neuron not-paraphrase logits:  [[ 0.537705  -2.2180324]]\n"
     ]
    }
   ],
   "source": [
    "# Load the TorchScript compiled model\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Verify the TorchScript works on both example inputs\n",
    "neuron_paraphrase_logits = model_neuron(*paraphrase)[0]\n",
    "neuron_not_paraphrase_logits = model_neuron(*not_paraphrase)[0]\n",
    "\n",
    "# Compare the results\n",
    "print('CPU paraphrase logits:        ', cpu_paraphrase_logits.detach().numpy())\n",
    "print('Neuron paraphrase logits:    ', neuron_paraphrase_logits.detach().numpy())\n",
    "print('CPU not-paraphrase logits:    ', cpu_not_paraphrase_logits.detach().numpy())\n",
    "print('Neuron not-paraphrase logits: ', neuron_not_paraphrase_logits.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4553cc9",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "In this section we benchmark the performance of the BERT model on Neuron. By default, models compiled with `torch_neuronx` will always execute on a *single* NeuronCore. When loading *multiple* models, the default behavior of the Neuron runtime is to evenly distribute models across all available NeuronCores. The runtime places models on the NeuronCore that has the fewest models loaded to it first. In the following section, we will `torch.jit.load` multiple instances of the model which should each be loaded onto their own NeuronCore. It is not useful to load more copies of a model than the number of NeuronCores on the instance since an individual NeuronCore can only execute one model at a time.\n",
    "\n",
    "To ensure that we are maximizing hardware utilization, we must run inferences using multiple threads in parallel. It is nearly always recommended to use some form of threading/multiprocessing and some form of model replication since even the smallest Neuron EC2 instance has 2 NeuronCores available. Applications with no form of threading are only capable of `1 / num_neuron_cores` hardware utilization which becomes especially problematic on large instances.\n",
    "\n",
    "One way to view the hardware utilization is by executing the `neuron-top` application in the terminal while the benchmark is executing. If the monitor shows >90% utilization on all NeuronCores, this is a good indication that the hardware is being utilized effectively.\n",
    "\n",
    "In this example we load two models, which utilizes all NeuronCores (2) on a `trn1.2xlarge` or `inf2.xlarge` instance. Additional models can be loaded and run in parallel on larger Trn1 or Inf2 instance sizes to increase throughput.\n",
    "\n",
    "We define a benchmarking function that loads two optimized BERT models onto two separate NeuronCores, runs multithreaded inference, and calculates the corresponding latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e14b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark(filename, example, n_models=2, n_threads=2, batches_per_thread=1000):\n",
    "    \"\"\"\n",
    "    Record performance statistics for a serialized model and its input example.\n",
    "\n",
    "    Arguments:\n",
    "        filename: The serialized torchscript model to load for benchmarking.\n",
    "        example: An example model input.\n",
    "        n_models: The number of models to load.\n",
    "        n_threads: The number of simultaneous threads to execute inferences on.\n",
    "        batches_per_thread: The number of example batches to run per thread.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of performance statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load models\n",
    "    models = [torch.jit.load(filename) for _ in range(n_models)]\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(8):\n",
    "        for model in models:\n",
    "            model(*example)\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    # Thread task\n",
    "    def task(model):\n",
    "        for _ in range(batches_per_thread):\n",
    "            start = time.time()\n",
    "            model(*example)\n",
    "            finish = time.time()\n",
    "            latencies.append((finish - start) * 1000)\n",
    "\n",
    "    # Submit tasks\n",
    "    begin = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_threads) as pool:\n",
    "        for i in range(n_threads):\n",
    "            pool.submit(task, models[i % len(models)])\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute metrics\n",
    "    boundaries = [50, 95, 99]\n",
    "    percentiles = {}\n",
    "\n",
    "    for boundary in boundaries:\n",
    "        name = f'latency_p{boundary}'\n",
    "        percentiles[name] = np.percentile(latencies, boundary)\n",
    "    duration = end - begin\n",
    "    batch_size = 0\n",
    "    for tensor in example:\n",
    "        if batch_size == 0:\n",
    "            batch_size = tensor.shape[0]\n",
    "    inferences = len(latencies) * batch_size\n",
    "    throughput = inferences / duration\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'filename': str(filename),\n",
    "        'batch_size': batch_size,\n",
    "        'batches': len(latencies),\n",
    "        'inferences': inferences,\n",
    "        'threads': n_threads,\n",
    "        'models': n_models,\n",
    "        'duration': duration,\n",
    "        'throughput': throughput,\n",
    "        **percentiles,\n",
    "    }\n",
    "\n",
    "    display(metrics)\n",
    "\n",
    "\n",
    "def display(metrics):\n",
    "    \"\"\"\n",
    "    Display the metrics produced by `benchmark` function.\n",
    "\n",
    "    Args:\n",
    "        metrics: A dictionary of performance statistics.\n",
    "    \"\"\"\n",
    "    pad = max(map(len, metrics)) + 1\n",
    "    for key, value in metrics.items():\n",
    "\n",
    "        parts = key.split('_')\n",
    "        parts = list(map(str.title, parts))\n",
    "        title = ' '.join(parts) + \":\"\n",
    "\n",
    "        if isinstance(value, float):\n",
    "            value = f'{value:0.3f}'\n",
    "\n",
    "        print(f'{title :<{pad}} {value}')\n",
    "\n",
    "\n",
    "# Benchmark BERT on Neuron\n",
    "benchmark(filename, paraphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc374b12",
   "metadata": {},
   "source": [
    "## Finding the optimal batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113acb55",
   "metadata": {},
   "source": [
    "Batch size has a direct impact on model performance. The NeuronCore architecture is optimized to maximize throughput with relatively small batch sizes. This means that a Neuron compiled model can outperform a GPU model, even if running single digit batch sizes.\n",
    "\n",
    "As a general best practice, we recommend optimizing your model‚Äôs throughput by compiling the model with a small batch size and gradually increasing it to find the peak throughput on Neuron. To minimize latency, using `batch size = 1` will nearly always be optimal. This batch size configuration is typically used for on-demand inference applications. To maximize throughput, *usually* `1 < batch_size < 10` is optimal. A configuration which uses a larger batch size is generally ideal for batched on-demand inference or offline batch processing.\n",
    "\n",
    "In the following section, we compile BERT for multiple batch size inputs. We then run inference on each batch size and benchmark the performance. Notice that latency increases consistently as the batch size increases. Throughput increases as well, up until a certain point where the input size becomes too large to be efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be26aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "Compiler status PASS\n",
      "..\n",
      "Compiler status PASS\n",
      "..\n",
      "Compiler status PASS\n",
      "..\n",
      "Compiler status PASS\n",
      "...\n",
      "Compiler status PASS\n",
      "..\n",
      "Compiler status PASS\n",
      ".."
     ]
    }
   ],
   "source": [
    "# Compile BERT for different batch sizes\n",
    "for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(name, torchscript=True)\n",
    "    example = encode(tokenizer, sequence_0, sequence_2, batch_size=batch_size)\n",
    "    model_neuron = torch_neuronx.trace(model, example)\n",
    "    filename = f'model_batch_size_{batch_size}.pt'\n",
    "    torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0f6ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_1.pt\n",
      "Batch Size:  1\n",
      "Batches:     2000\n",
      "Inferences:  2000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    0.969\n",
      "Throughput:  2063.669\n",
      "Latency P50: 0.968\n",
      "Latency P95: 0.974\n",
      "Latency P99: 0.982\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_2.pt\n",
      "Batch Size:  2\n",
      "Batches:     2000\n",
      "Inferences:  4000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.470\n",
      "Throughput:  2721.648\n",
      "Latency P50: 1.467\n",
      "Latency P95: 1.476\n",
      "Latency P99: 1.499\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_3.pt\n",
      "Batch Size:  3\n",
      "Batches:     2000\n",
      "Inferences:  6000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    1.911\n",
      "Throughput:  3139.059\n",
      "Latency P50: 1.910\n",
      "Latency P95: 1.917\n",
      "Latency P99: 1.924\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_4.pt\n",
      "Batch Size:  4\n",
      "Batches:     2000\n",
      "Inferences:  8000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    2.454\n",
      "Throughput:  3260.260\n",
      "Latency P50: 2.464\n",
      "Latency P95: 2.469\n",
      "Latency P99: 2.476\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_5.pt\n",
      "Batch Size:  5\n",
      "Batches:     2000\n",
      "Inferences:  10000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    3.024\n",
      "Throughput:  3307.169\n",
      "Latency P50: 3.020\n",
      "Latency P95: 3.043\n",
      "Latency P99: 3.052\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_6.pt\n",
      "Batch Size:  6\n",
      "Batches:     2000\n",
      "Inferences:  12000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    3.629\n",
      "Throughput:  3307.083\n",
      "Latency P50: 3.629\n",
      "Latency P95: 3.637\n",
      "Latency P99: 3.646\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_7.pt\n",
      "Batch Size:  7\n",
      "Batches:     2000\n",
      "Inferences:  14000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    4.122\n",
      "Throughput:  3396.757\n",
      "Latency P50: 4.127\n",
      "Latency P95: 4.137\n",
      "Latency P99: 4.142\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_8.pt\n",
      "Batch Size:  8\n",
      "Batches:     2000\n",
      "Inferences:  16000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    4.638\n",
      "Throughput:  3450.102\n",
      "Latency P50: 4.634\n",
      "Latency P95: 4.652\n",
      "Latency P99: 4.656\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_9.pt\n",
      "Batch Size:  9\n",
      "Batches:     2000\n",
      "Inferences:  18000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    5.291\n",
      "Throughput:  3402.277\n",
      "Latency P50: 5.293\n",
      "Latency P95: 5.311\n",
      "Latency P99: 5.316\n",
      "\n",
      "--------------------------------------------------\n",
      "Filename:    model_batch_size_10.pt\n",
      "Batch Size:  10\n",
      "Batches:     2000\n",
      "Inferences:  20000\n",
      "Threads:     2\n",
      "Models:      2\n",
      "Duration:    5.803\n",
      "Throughput:  3446.642\n",
      "Latency P50: 5.798\n",
      "Latency P95: 5.817\n",
      "Latency P99: 5.836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark BERT for different batch sizes\n",
    "for batch_size in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    print('-'*50)\n",
    "    example = encode(tokenizer, sequence_0, sequence_2, batch_size=batch_size)\n",
    "    filename = f'model_batch_size_{batch_size}.pt'\n",
    "    benchmark(filename, example)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
